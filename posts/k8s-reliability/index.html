<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.48" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>K8s Reliability doc from kubespray &middot; Lei Zhilong&#39;s Blog</title>

  
  <link type="text/css" rel="stylesheet" href="https://leizhilong.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://leizhilong.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://leizhilong.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://leizhilong.github.io/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="Lei Zhilong&#39;s Blog" />

  
</head>

  <body class=" layout-reverse">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://leizhilong.github.io/"><h1>Lei Zhilong&#39;s Blog</h1></a>
      <p class="lead">
       Just some notes 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://leizhilong.github.io/">Home</a> </li>
        <li><a href="/about"> About </a></li>
      </ul>
    </nav>

    <p>All Rights Reserved © 2018</p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1>K8s Reliability doc from kubespray</h1>
  <span class="post-date">Thu, Sep 6, 2018</span>
  

<h1 id="reference">Reference</h1>

<p><a href="https://github.com/kubernetes-incubator/kubespray/blob/master/docs/kubernetes-reliability.md">https://github.com/kubernetes-incubator/kubespray/blob/master/docs/kubernetes-reliability.md</a></p>

<h1 id="overview">Overview</h1>

<p>Distributed system such as Kubernetes are designed to be resilient to the
failures.  More details about Kubernetes High-Availability (HA) may be found at
<a href="https://kubernetes.io/docs/admin/high-availability/">Building High-Availability Clusters</a></p>

<p>To have a simple view the most of parts of HA will be skipped to describe
Kubelet&lt;-&gt;Controller Manager communication only.</p>

<p>By default the normal behavior looks like:</p>

<ol>
<li><p>Kubelet updates it status to apiserver periodically, as specified by
<code>--node-status-update-frequency</code>. The default value is <strong>10s</strong>.</p></li>

<li><p>Kubernetes controller manager checks the statuses of Kubelets every
<code>–-node-monitor-period</code>. The default value is <strong>5s</strong>.</p></li>

<li><p>In case the status is updated  within <code>--node-monitor-grace-period</code> of time,
Kubernetes controller manager considers healthy status of Kubelet. The
default value is <strong>40s</strong>.</p></li>
</ol>

<blockquote>
<p>Kubernetes controller manager and Kubelets work asynchronously. It means that
the delay may include any network latency, API Server latency, etcd latency,
latency caused by load on one&rsquo;s master nodes and so on. So if
<code>--node-status-update-frequency</code> is set to 5s in reality it may appear in
etcd in 6-7 seconds or even longer when etcd cannot commit data to quorum
nodes.</p>
</blockquote>

<h1 id="failure">Failure</h1>

<p>Kubelet will try to make <code>nodeStatusUpdateRetry</code> post attempts. Currently
<code>nodeStatusUpdateRetry</code> is constantly set to 5 in
<a href="https://github.com/kubernetes/kubernetes/blob/release-1.5/pkg/kubelet/kubelet.go#L102">kubelet.go</a>.</p>

<p>Kubelet will try to update the status in
<a href="https://github.com/kubernetes/kubernetes/blob/release-1.5/pkg/kubelet/kubelet_node_status.go#L312">tryUpdateNodeStatus</a>
function. Kubelet uses <code>http.Client()</code> Golang method, but has no specified
timeout. Thus there may be some glitches when API Server is overloaded while
TCP connection is established.</p>

<p>So, there will be <code>nodeStatusUpdateRetry</code> * <code>--node-status-update-frequency</code>
attempts to set a status of node.</p>

<p>At the same time Kubernetes controller manager will try to check
<code>nodeStatusUpdateRetry</code> times every <code>--node-monitor-period</code> of time. After
<code>--node-monitor-grace-period</code> it will consider node unhealthy. It will remove
its pods based on <code>--pod-eviction-timeout</code></p>

<p>Kube proxy has a watcher over API. Once pods are evicted, Kube proxy will
notice and will update iptables of the node. It will remove endpoints from
services so pods from failed node won&rsquo;t be accessible anymore.</p>

<h1 id="recommendations-for-different-cases">Recommendations for different cases</h1>

<h2 id="fast-update-and-fast-reaction">Fast Update and Fast Reaction</h2>

<p>If <code>-–node-status-update-frequency</code> is set to <strong>4s</strong> (10s is default).
<code>--node-monitor-period</code> to <strong>2s</strong> (5s is default).
<code>--node-monitor-grace-period</code> to <strong>20s</strong> (40s is default).
<code>--pod-eviction-timeout</code> is set to <strong>30s</strong> (5m is default)</p>

<p>In such scenario, pods will be evicted in <strong>50s</strong> because the node will be
considered as down after <strong>20s</strong>, and <code>--pod-eviction-timeout</code> occurs after
<strong>30s</strong> more.  However, this scenario creates an overhead on etcd as every node
will try to update its status every 2 seconds.</p>

<p>If the environment has 1000 nodes, there will be 15000 node updates per
minute which may require large etcd containers or even dedicated nodes for etcd.</p>

<blockquote>
<p>If we calculate the number of tries, the division will give 5, but in reality
it will be from 3 to 5 with <code>nodeStatusUpdateRetry</code> attempts of each try. The
total number of attemtps will vary from 15 to 25 due to latency of all
components.</p>
</blockquote>

<h2 id="medium-update-and-average-reaction">Medium Update and Average Reaction</h2>

<p>Let&rsquo;s set <code>-–node-status-update-frequency</code> to <strong>20s</strong>
<code>--node-monitor-grace-period</code> to <strong>2m</strong> and <code>--pod-eviction-timeout</code> to <strong>1m</strong>.
In that case, Kubelet will try to update status every 20s. So, it will be 6 * 5
= 30 attempts before Kubernetes controller manager will consider unhealthy
status of node. After 1m it will evict all pods. The total time will be 3m
before eviction process.</p>

<p>Such scenario is good for medium environments as 1000 nodes will require 3000
etcd updates per minute.</p>

<blockquote>
<p>In reality, there will be from 4 to 6 node update tries. The total number of
of attempts will vary from 20 to 30.</p>
</blockquote>

<h2 id="low-update-and-slow-reaction">Low Update and Slow reaction</h2>

<p>Let&rsquo;s set <code>-–node-status-update-frequency</code> to <strong>1m</strong>.
<code>--node-monitor-grace-period</code> will set to <strong>5m</strong> and <code>--pod-eviction-timeout</code>
to <strong>1m</strong>. In this scenario, every kubelet will try to update the status every
minute. There will be 5 * 5 = 25 attempts before unhealty status. After 5m,
Kubernetes controller manager will set unhealthy status. This means that pods
will be evicted after 1m after being marked unhealthy. (6m in total).</p>

<blockquote>
<p>In reality, there will be from 3 to 5 tries. The total number of attempt will
vary from 15 to 25.</p>
</blockquote>

<p>There can be different combinations such as Fast Update with Slow reaction to
satisfy specific cases.</p>

</div>


    </main>

    
  </body>
</html>
